{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea36282a",
   "metadata": {},
   "source": [
    "# Exercise 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727fb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://cernbox.cern.ch/s/6Ec5pGFEpFWeH6S/download -o ./Data-MLtutorial.tar.gz\n",
    "! tar -xvzf ./Data-MLtutorial.tar.gz -C ./\n",
    "! rm ./Data-MLtutorial.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cedb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "target = np.array([])\n",
    "inputs = np.array([])\n",
    "\n",
    "datafiles = ['./Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5']\n",
    "\n",
    "for file_ in datafiles:\n",
    "    with h5py.File(file_, 'r') as f:\n",
    "        print(\"Appending {}\".format(file_))\n",
    "        jets = np.array(f.get('jets'))\n",
    "        tmp_inputs = np.array(f.get(\"jets\"))[:,[5, 10]] # That's `j_tau2_b1` and `j_tau32_b1`\n",
    "        tmp_target = np.array(f.get('jets'))[:,6] # That's `j_tau3_b1`\n",
    "        inputs = np.concatenate([inputs, tmp_inputs], axis=0) if inputs.size else tmp_inputs\n",
    "        target = np.concatenate([target, tmp_target], axis=0) if target.size else tmp_target\n",
    "\n",
    "inputs = inputs / 100. # Prevents numerical issues\n",
    "print(target.shape, inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80737611",
   "metadata": {},
   "source": [
    "Let's try to implement an linear regression that uses [gradient descent](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html) to optimize its parameters. Use **(Mean Square Error)** as the objective to minimize, given by: $L(y, f(x)) = \\frac{1}{2n} \\sum\\limits^{n} (y - w^TX)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefcd60",
   "metadata": {},
   "source": [
    "**Methods to implement (in order):**\n",
    "\n",
    "- `__init__` - initializes class with `coef_` and `intercept_` attributes set to `None`.\n",
    "- `init_weights` - accepts an input `num_features`. Sets the `coef_` and `intercept_` attributes to random values drawn from a normal distribution with mean 0 and standard deviation 1 using `np.random.normal`. Use a numpy array for `coef_` and a float for `intercept_`.\n",
    "- `predict` - accepts an input `X` and outputs a prediction numpy array using the current parameters.\n",
    "- `score` - accepts inputs `X` and `y_true` and outputs the $R^2$ score of the model.\n",
    "- `calc_loss` - accepts inputs `X` and `y_true` as numpy arrays. Calculates and returns the Mean Square Error loss metric with the current parameters.\n",
    "- `calc_grad` - accepts inputs `X` and `y_true` and, using the current parameters, outputs the gradients ([coef_grad, intercept_grad]). Does not update the parameters.\n",
    "- `fit` - accepts inputs `X`, `y_true` and default kwargs `max_iter`, `learning_rate` and fits a model using gradient descent. Should call `calc_grad` to get gradients from current parameters and update the parameters using the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel():\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes class with coef_ and intercept_ attributes set to None.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def init_weights(self, num_features):\n",
    "        \"\"\"Accepts an input num_features.\n",
    "        \n",
    "        Sets the coef_ and intercept_ attributes to random values\n",
    "        drawn from a normal distribution with mean 0 and standard\n",
    "        deviation 1 using np.random.normal. Use a numpy array for\n",
    "        coef_ and a float for intercept_.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Creates a prediction with the current coef/intercept values\"\"\"\n",
    "        pass\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        \"\"\"Accepts inputs X and y_true and outputs the R2 score of the model.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def calc_loss(self, X, y_true):\n",
    "        \"\"\"Calculates the loss value using current coef_ and intercept_ values\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def calc_grad(self, X, y_true):\n",
    "        \"\"\"Calculates gradients for coef/intercept values\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y_true, max_iter=10000, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Accepts inputs X, y_true and default kwargs max_iter,\n",
    "        learning_rate. Fits a model using gradient descent.\n",
    "        Should call calc_grad to get gradients from current\n",
    "        parameters and update the parameters using the gradient.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a141324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_regressor = LinearRegressionModel()\n",
    "my_regressor.init_weights(2)\n",
    "my_regressor.fit(inputs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(target, my_regressor.predict(inputs))\n",
    "plt.plot([0, 100], [0, 100], 'r')\n",
    "plt.xlabel('Expected')\n",
    "plt.ylabel('Predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The resulting equation: {}x + {} has a R2 score of {}\".format(\n",
    "    my_regressor.coef_,\n",
    "    my_regressor.intercept_,\n",
    "    my_regressor.score(inputs, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119d818",
   "metadata": {},
   "source": [
    "**Expected R2: 0.73**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ad3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(inputs, target)\n",
    "print(\"The resulting equation: {}x + {} has a R2 score of {}\".format(\n",
    "    reg.coef_,\n",
    "    reg.intercept_,\n",
    "    reg.score(inputs, target)))\n",
    "plt.scatter(target, my_regressor.predict(inputs))\n",
    "plt.plot([0, 100], [0, 100], 'r')\n",
    "plt.xlabel('Expected')\n",
    "plt.ylabel('Predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4aacae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
